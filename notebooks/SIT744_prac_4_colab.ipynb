{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SIT744_prac_4_colab.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ZeHe2I9Ix2iM"},"source":["# SIT744 Practical 4: Second look at TensorFlow and Keras\n","\n","\n","*Prof. Antonio Robles-Kelly*\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"l--dd-g0DOiD"},"source":["<div class=\"alert alert-info\">\n","We suggest that you run this notebook using Google Colab.\n","</div>\n","\n","## Pre-practical readings\n","\n","- [TensorFlow tensors](https://www.tensorflow.org/guide/tensor)\n","- [TensorFlow Variables](https://www.tensorflow.org/guide/variable)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6qqyuRZ3GmPj"},"source":["## Task 1. Low-level tensor manipulation in TensorFlow\n","\n","TensorFlow APIs cover three deep-learning components:\n","\n","- Tensors, including variables for keeping layer weights\n","- Tensor operations, including tensor multiplication,  addition, and the activation functions\n","- Backpropagation, implemented through the GradientTape object\n","\n","In this task, we will learn how to use these TensorFlow components. "]},{"cell_type":"markdown","metadata":{"id":"TJVIrhwYGwCg"},"source":["### Task 1.1 Tensors and tensor operations"]},{"cell_type":"markdown","metadata":{"id":"Za_ox3Xeit20"},"source":["\n","\n","#### tf.Tensor vs tensors\n","\n","We mentioned that tensors (with lower-case t) are like NumPy arrays. However, [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) (with upper-case T) is different. A tf.Tensor defines a function (or computation) that, when called, produces a tensor. Such a function is called an **Operator** in the TensorFlow nomenclature.\n","\n","Therefore, a TensorFlow program forms a computation graph that chains a collection of tf.Tensor's together. You can think that the program  itself contains no data. When we invoke the program, data (tensors) are generated or passed in and flow through operators in the program. But how can we store the model parameters and train the model parameters?"]},{"cell_type":"markdown","metadata":{"id":"3x8D8V-2ixES"},"source":["\n","\n","#### Constant tensors and Variables\n","\n","Assuming that we separate data from computation. Then `tf.constant` and `tf.Variable` are the two main `tf.Tensor` that directly deal with data. Data in `tf.constant` is immutable and data in `tf.Variable` can be changed.\n","\n","Constant tensors return the same data at every invocation of the computation graph.\n"]},{"cell_type":"code","metadata":{"id":"tq3b2gtaRFxe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626925440070,"user_tz":-600,"elapsed":8003,"user":{"displayName":"Antonio Robles-Kelly","photoUrl":"","userId":"12004615958940169307"}},"outputId":"10cf52b1-76bb-46b8-8da4-e28c641d92a4"},"source":["\n","import tensorflow as tf\n","\n","a = tf.constant([[1, 2],\n","                 [3, 4]])\n","print(a)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tf.Tensor(\n","[[1 2]\n"," [3 4]], shape=(2, 2), dtype=int32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1sUVaVzpWy94"},"source":["As you can see that `tf.constant` returns a tensor. There are some other functions that return constant tensors."]},{"cell_type":"code","metadata":{"id":"jkhiymqTWbAl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626925440071,"user_tz":-600,"elapsed":12,"user":{"displayName":"Antonio Robles-Kelly","photoUrl":"","userId":"12004615958940169307"}},"outputId":"0e791f78-6ec8-4cfd-ad7d-ad4f3bed3018"},"source":["x = tf.ones(shape=(2, 2))\n","print(x)\n","x = tf.zeros(shape=(2, 2))\n","print(x)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tf.Tensor(\n","[[1. 1.]\n"," [1. 1.]], shape=(2, 2), dtype=float32)\n","tf.Tensor(\n","[[0. 0.]\n"," [0. 0.]], shape=(2, 2), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PqERjp2DYL1z"},"source":["A particularly important class of functions are those used for generating random initial weights in a network. They produced constant tensors as the initial values for variables.  "]},{"cell_type":"code","metadata":{"id":"Q-MAXnakY_-5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626925440072,"user_tz":-600,"elapsed":11,"user":{"displayName":"Antonio Robles-Kelly","photoUrl":"","userId":"12004615958940169307"}},"outputId":"f6614ebe-ca52-41b1-e6bd-d6f85c8ac426"},"source":["## Normal initialiser\n","w_init = tf.random_normal_initializer(\n","    mean=0.0, stddev=0.05, seed=None\n",")\n","\n","initial_value=w_init(shape=(2, 2),\n","                     dtype='float32')\n","\n","print(initial_value)\n","\n","\n","## Uniform initialiser\n","w_init = tf.random_uniform_initializer(\n","    minval = -0.05, maxval = 0.05, seed=None\n",")\n","\n","initial_value=w_init(shape=(2, 2),\n","                     dtype='float32')\n","\n","print(initial_value)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tf.Tensor(\n","[[-0.10802678 -0.01151871]\n"," [ 0.10591703  0.08391316]], shape=(2, 2), dtype=float32)\n","tf.Tensor(\n","[[-0.00666878  0.04656264]\n"," [ 0.02674488  0.03010588]], shape=(2, 2), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AyEx_1Lyf09n"},"source":["**exercise** Try to assign a new value to a constant tensor. Can you do that?"]},{"cell_type":"markdown","metadata":{"id":"4daBVDTUgES4"},"source":["In comparison to constants, variables can be assigned a different value. They are required to represent trainable network/layer weights"]},{"cell_type":"code","metadata":{"id":"oO5-Vy5Cgk98","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626925440072,"user_tz":-600,"elapsed":8,"user":{"displayName":"Antonio Robles-Kelly","photoUrl":"","userId":"12004615958940169307"}},"outputId":"006fda9e-e948-49f4-d034-f5a3ec7cdcaa"},"source":["w = tf.Variable(initial_value=initial_value,\n","                         trainable=True)\n","print(w)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n","array([[-0.00666878,  0.04656264],\n","       [ 0.02674488,  0.03010588]], dtype=float32)>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HnsodVqShn0H"},"source":["You can use the `assign` function to change the value in a variable."]},{"cell_type":"code","metadata":{"id":"gILckVTthCgY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626925440073,"user_tz":-600,"elapsed":7,"user":{"displayName":"Antonio Robles-Kelly","photoUrl":"","userId":"12004615958940169307"}},"outputId":"b9a155cd-1401-4f81-bb3b-7433a11158ed"},"source":["print(w[0,0])\n","\n","\n","w[0,0].assign(0)\n","print(w[0,0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tf.Tensor(-0.0066687837, shape=(), dtype=float32)\n","tf.Tensor(0.0, shape=(), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IOXmlZ0ai_uo"},"source":["### Task 1.2 Math operations in TensorFlow\n","\n","The transformation of tensors are achieved by matrix multiplications, additions, reshaping, and activation functions."]},{"cell_type":"code","metadata":{"id":"w2st7tlXj3_K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626925442759,"user_tz":-600,"elapsed":2692,"user":{"displayName":"Antonio Robles-Kelly","photoUrl":"","userId":"12004615958940169307"}},"outputId":"632aaba4-e2b7-4ba1-e5ad-6d64eeaf1f27"},"source":["a = tf.ones((2, 2))\n","print(f'a: {a}')\n","b = tf.square(a)\n","print(f'b: {b}')\n","c = tf.sqrt(a)\n","print(f'c: {c}')\n","d = b + c\n","print(f'd = b + c: {d}')\n","e = tf.matmul(a, b)\n","print(f'e = tf.matmul(a, b): {e}')\n","e *= d\n","print(f'e *= d: {e}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["a: [[1. 1.]\n"," [1. 1.]]\n","b: [[1. 1.]\n"," [1. 1.]]\n","c: [[1. 1.]\n"," [1. 1.]]\n","d = b + c: [[2. 2.]\n"," [2. 2.]]\n","e = tf.matmul(a, b): [[2. 2.]\n"," [2. 2.]]\n","e *= d: [[4. 4.]\n"," [4. 4.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EtjaWP2KlwIG"},"source":["### Task 1.3 Performing differentiation with tensor operations\n","\n","Tensor operations come with the ability to perform automatic differentiation."]},{"cell_type":"code","metadata":{"id":"JztuHNjymED5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626925442760,"user_tz":-600,"elapsed":11,"user":{"displayName":"Antonio Robles-Kelly","photoUrl":"","userId":"12004615958940169307"}},"outputId":"619a0932-6cdc-485f-a5c4-7e87f6737f73"},"source":["x = tf.Variable(initial_value= tf.ones((2, 2)))\n","\n","with tf.GradientTape() as tape:\n","   y = tf.square(x)\n","dy_dx = tape.gradient(y, x)\n","print(dy_dx)\n","\n","with tf.GradientTape() as paper:\n","   z = tf.sqrt(x)\n","\n","dz_dx = paper.gradient(z, [x])\n","\n","\n","print(dz_dx)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tf.Tensor(\n","[[2. 2.]\n"," [2. 2.]], shape=(2, 2), dtype=float32)\n","[<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n","array([[0.5, 0.5],\n","       [0.5, 0.5]], dtype=float32)>]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"l1vdti4sm_ET"},"source":["**exercise** Modify the code above to compute the gradient of the exponential function $y=e^{x}$.\n","\n","**question** Can you call `tape.gradient` twice?\n"]},{"cell_type":"markdown","metadata":{"id":"NVBPzC3pCFeH"},"source":["## Task 2  Reimplementing a Keras model in TensorFlow\n","\n","Last week, we used a Keras model to run through the MNIST example. In this practical, we learn how to reimplement the model without using Keras. This will deepen your understanding of some key concepts.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VFn0BndZDORM"},"source":["### Task 2.1 A simple Dense class\n","\n","We know that a dense layer is essentially performing an affine transformation followed by an activation function.\n","\n","> output = activation(dot(W, input) + b)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZgkoyWBOY3Y5"},"source":["We can define a class for network layers.\n"]},{"cell_type":"code","metadata":{"id":"_OiUXe6qDtpA"},"source":["class NaiveDense:\n","\n","    def __init__(self, units, input_dim, activation):\n","        self.activation = activation\n","\n","        W_init = tf.random_normal_initializer()\n","        self.W = tf.Variable(initial_value=W_init(shape=(input_dim, units),\n","                                              dtype='float32'),\n","                         trainable=True)\n","\n","        b_init = tf.zeros_initializer()\n","        self.b = tf.Variable(initial_value=b_init(shape=(units,),\n","                                              dtype='float32'),\n","                         trainable=True)\n","        \n","    def __call__(self, inputs):\n","        return self.activation(tf.matmul(inputs, self.W) + self.b)\n","\n","    @property\n","    def weights(self):\n","        return [self.W, self.b]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CwVnV45HZNGA"},"source":["Let's try to pass a tensor through the layer."]},{"cell_type":"code","metadata":{"id":"EcZK6Ik91hYM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626925442761,"user_tz":-600,"elapsed":10,"user":{"displayName":"Antonio Robles-Kelly","photoUrl":"","userId":"12004615958940169307"}},"outputId":"8584ea8f-76fc-4938-b2ec-4b1dfd6b2ab6"},"source":["relu_layer = NaiveDense(units=10, input_dim=2, activation = tf.nn.relu)\n","\n","x = tf.ones((2, 2))\n","y = relu_layer(x)\n","print(y)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tf.Tensor(\n","[[0.05805763 0.         0.11744042 0.         0.08183253 0.00523644\n","  0.         0.01771953 0.         0.03236277]\n"," [0.05805763 0.         0.11744042 0.         0.08183253 0.00523644\n","  0.         0.01771953 0.         0.03236277]], shape=(2, 10), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mUc4le2VRI_r"},"source":["You can compare this with the original implementation from Keras. The difference in values is due to random initialisation."]},{"cell_type":"code","metadata":{"id":"y-P3mu82Q-Fu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626925442761,"user_tz":-600,"elapsed":8,"user":{"displayName":"Antonio Robles-Kelly","photoUrl":"","userId":"12004615958940169307"}},"outputId":"172a3a29-c4cc-41fa-e8f7-434bf1ad5d70"},"source":["keras_layer = tf.keras.layers.Dense(units=10, activation = tf.nn.relu)\n","y = keras_layer(x)\n","print(y)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tf.Tensor(\n","[[0.         1.3277433  0.         0.7832881  0.         0.81222737\n","  0.02711302 0.         0.64552873 0.        ]\n"," [0.         1.3277433  0.         0.7832881  0.         0.81222737\n","  0.02711302 0.         0.64552873 0.        ]], shape=(2, 10), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UmTP7AnDMTx3"},"source":["**question** Do you see negative values in the output? Why? Are we using the 10 output units effectively?"]},{"cell_type":"markdown","metadata":{"id":"vFIXRAjmedgo"},"source":["### Task 2.2 A simple Sequential class\n","\n","Once we have defined some layers, we can chain them together. Let's define a class similar to the Sequential Model in Keras.\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"pA_l3xFOZ2Ra"},"source":["class NaiveSequential:\n","\n","    def __init__(self, layers):\n","        self.layers = layers\n","\n","    def __call__(self, inputs):\n","        x = inputs\n","        for layer in self.layers:\n","           x = layer(x)\n","        return x\n","\n","    @property\n","    def weights(self):\n","       weights = []\n","       for layer in self.layers:\n","           weights += layer.weights\n","       return weights"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WJBlvhAAZ3KY"},"source":["It takes a list of layers and returns a model."]},{"cell_type":"code","metadata":{"id":"qJfzpqFSYEAa"},"source":["model = NaiveSequential([\n","    NaiveDense(units=512, input_dim=28 * 28, activation=tf.nn.relu),\n","    NaiveDense(units=10, input_dim=512,  activation=tf.nn.softmax)\n","])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gtYRMuhUazKf"},"source":["Let's try to feed the model two identical \"images\".\n"]},{"cell_type":"code","metadata":{"id":"oVcWreS4aPTu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626925443724,"user_tz":-600,"elapsed":14,"user":{"displayName":"Antonio Robles-Kelly","photoUrl":"","userId":"12004615958940169307"}},"outputId":"ab148f39-87ae-48e2-b446-8548e3861d6b"},"source":["model(tf.ones((2, 28 * 28)))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n","array([[0.01229792, 0.04085715, 0.2795372 , 0.02992687, 0.10242704,\n","        0.16270003, 0.01662579, 0.30111566, 0.02210492, 0.03240742],\n","       [0.01229792, 0.04085715, 0.2795372 , 0.02992687, 0.10242704,\n","        0.16270003, 0.01662579, 0.30111566, 0.02210492, 0.03240742]],\n","      dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"La67OKRHGtH0"},"source":["### Task 2.3 A batch generator\n","\n","To run stochastic gradient-descent, we need to feed the model with mini-batches of the input data. Later on, we will learn how to build TensorFlow input pipelines with `tf.data`. Here we will create a simple iterator for retrieving training batches."]},{"cell_type":"code","metadata":{"id":"m-3G9JI3b0is"},"source":["class BatchGenerator:\n","\n","    def __init__(self, images, labels, batch_size=128):\n","        self.index = 0\n","        self.images = images\n","        self.labels = labels\n","        self.batch_size = batch_size\n","\n","    def next(self):\n","        images = self.images[self.index : self.index + self.batch_size]\n","        labels = self.labels[self.index : self.index + self.batch_size]\n","        self.index += self.batch_size\n","        return images, labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZS9WrR5icG4G"},"source":["Let's try it on the MNIST data."]},{"cell_type":"code","metadata":{"id":"pAKNUkujZwTG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626925443725,"user_tz":-600,"elapsed":14,"user":{"displayName":"Antonio Robles-Kelly","photoUrl":"","userId":"12004615958940169307"}},"outputId":"d31a9858-a5d2-4f2c-cd3a-a61c6e8a9089"},"source":["from tensorflow.keras.datasets import mnist\n","\n","(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n","\n","train_images = train_images.reshape((60000, 28 * 28))\n","train_images = train_images.astype('float32') / 255\n","test_images = test_images.reshape((10000, 28 * 28))\n","test_images = test_images.astype('float32') / 255\n","\n","batch_generator = BatchGenerator(train_images, train_labels)\n","x, y = batch_generator.next()\n","print(f'x: {x}')\n","print(f'y: {y}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","x: [[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]]\n","y: [5 0 4 1 9 2 1 3 1 4 3 5 3 6 1 7 2 8 6 9 4 0 9 1 1 2 4 3 2 7 3 8 6 9 0 5 6\n"," 0 7 6 1 8 7 9 3 9 8 5 9 3 3 0 7 4 9 8 0 9 4 1 4 4 6 0 4 5 6 1 0 0 1 7 1 6\n"," 3 0 2 1 1 7 9 0 2 6 7 8 3 9 0 4 6 7 4 6 8 0 7 8 3 1 5 7 1 7 1 1 6 3 0 2 9\n"," 3 1 1 0 4 9 2 0 0 2 0 2 7 1 8 6 4]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jJil0L1MyzIy"},"source":["## Task 3 Training the model\n","\n","As mentioned in the lecture. training a neural network involves a loop with the following steps:\n","\n","1. Compute the predictions of the examples in the batch\n","2. Compute the loss value for these predictions given the actual labels\n","3. Compute the gradient of the loss with regard to the model’s weights\n","4. Move the weights by a small amount in the direction opposite to the gradient\n","\n","These four steps comprise one **training step**."]},{"cell_type":"code","metadata":{"id":"5hVfO79afq98"},"source":["learning_rate = 1e-3\n","\n","def update_weights(gradients, weights):\n","    for g, w in zip(gradients, model.weights):\n","        w.assign_sub(g * learning_rate)\n","\n","def one_training_step(model, images_batch, labels_batch):\n","    with tf.GradientTape() as tape:\n","      predictions = model(images_batch) ## 1. Compute the predictions of the examples in the batch \n","      per_sample_losses = tf.keras.losses.sparse_categorical_crossentropy(\n","          labels_batch, predictions) \n","      average_loss = tf.reduce_mean(per_sample_losses) ## 2. Compute the loss value for these predictions given the actual labels\n","    gradients = tape.gradient(average_loss, model.weights) ## 3. Compute the gradient of the loss with regard to the model’s weights\n","    update_weights(gradients, model.weights) ## 4. Move the weights by a small amount in the direction opposite to the gradient\n","    return average_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"98d3KS-5hS9B"},"source":["Let's test-run it with a training batch."]},{"cell_type":"code","metadata":{"id":"jBSJibnBf6Cw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626925886924,"user_tz":-600,"elapsed":3,"user":{"displayName":"Antonio Robles-Kelly","photoUrl":"","userId":"12004615958940169307"}},"outputId":"5a121acb-4ed3-47da-ad88-c2fc3d5c5b61"},"source":["one_training_step(model, x, y)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=float32, numpy=0.49769723>"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"fkr_elRrhrbA"},"source":["Knowing that it is working, we can add a for-loop. Actually, we will use two nested for-loops: the outer for-loop to keep track of the number of epochs and the inner for-loop to iterate through the whole training data (multiple mini-batches). \n","\n"]},{"cell_type":"code","metadata":{"id":"DFr4pexgh88Z"},"source":["def fit(model, images, labels, epochs, batch_size=64):\n","    for epoch_counter in range(epochs):\n","      print('Epoch %d' % epoch_counter)\n","      batch_generator = BatchGenerator(images, labels)\n","      for batch_counter in range(len(images) // batch_size):\n","          images_batch, labels_batch = batch_generator.next()\n","          loss = one_training_step(model, images_batch, labels_batch)\n","          if batch_counter % 100 == 0:\n","              print('loss at batch %d: %.2f' % (batch_counter, loss))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tWxT0QKVbOEs"},"source":["Now we are ready to train the model. Specify 5 epochs."]},{"cell_type":"code","metadata":{"id":"KMZJ9kVMiBM5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626926138208,"user_tz":-600,"elapsed":112264,"user":{"displayName":"Antonio Robles-Kelly","photoUrl":"","userId":"12004615958940169307"}},"outputId":"19f0ead3-8904-44a0-eeb6-491074f48abe"},"source":["fit(model, train_images, train_labels, epochs=50, batch_size=128)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 0\n","loss at batch 0: 0.47\n","loss at batch 100: 0.47\n","loss at batch 200: 0.39\n","loss at batch 300: 0.48\n","loss at batch 400: 0.54\n","Epoch 1\n","loss at batch 0: 0.46\n","loss at batch 100: 0.45\n","loss at batch 200: 0.38\n","loss at batch 300: 0.47\n","loss at batch 400: 0.53\n","Epoch 2\n","loss at batch 0: 0.44\n","loss at batch 100: 0.44\n","loss at batch 200: 0.37\n","loss at batch 300: 0.46\n","loss at batch 400: 0.52\n","Epoch 3\n","loss at batch 0: 0.43\n","loss at batch 100: 0.43\n","loss at batch 200: 0.36\n","loss at batch 300: 0.45\n","loss at batch 400: 0.51\n","Epoch 4\n","loss at batch 0: 0.42\n","loss at batch 100: 0.42\n","loss at batch 200: 0.35\n","loss at batch 300: 0.44\n","loss at batch 400: 0.50\n","Epoch 5\n","loss at batch 0: 0.42\n","loss at batch 100: 0.41\n","loss at batch 200: 0.34\n","loss at batch 300: 0.43\n","loss at batch 400: 0.49\n","Epoch 6\n","loss at batch 0: 0.41\n","loss at batch 100: 0.40\n","loss at batch 200: 0.33\n","loss at batch 300: 0.42\n","loss at batch 400: 0.48\n","Epoch 7\n","loss at batch 0: 0.40\n","loss at batch 100: 0.39\n","loss at batch 200: 0.33\n","loss at batch 300: 0.42\n","loss at batch 400: 0.48\n","Epoch 8\n","loss at batch 0: 0.39\n","loss at batch 100: 0.38\n","loss at batch 200: 0.32\n","loss at batch 300: 0.41\n","loss at batch 400: 0.47\n","Epoch 9\n","loss at batch 0: 0.39\n","loss at batch 100: 0.37\n","loss at batch 200: 0.31\n","loss at batch 300: 0.40\n","loss at batch 400: 0.46\n","Epoch 10\n","loss at batch 0: 0.38\n","loss at batch 100: 0.37\n","loss at batch 200: 0.31\n","loss at batch 300: 0.40\n","loss at batch 400: 0.46\n","Epoch 11\n","loss at batch 0: 0.37\n","loss at batch 100: 0.36\n","loss at batch 200: 0.30\n","loss at batch 300: 0.39\n","loss at batch 400: 0.45\n","Epoch 12\n","loss at batch 0: 0.37\n","loss at batch 100: 0.36\n","loss at batch 200: 0.30\n","loss at batch 300: 0.39\n","loss at batch 400: 0.45\n","Epoch 13\n","loss at batch 0: 0.36\n","loss at batch 100: 0.35\n","loss at batch 200: 0.29\n","loss at batch 300: 0.38\n","loss at batch 400: 0.44\n","Epoch 14\n","loss at batch 0: 0.36\n","loss at batch 100: 0.34\n","loss at batch 200: 0.29\n","loss at batch 300: 0.38\n","loss at batch 400: 0.44\n","Epoch 15\n","loss at batch 0: 0.35\n","loss at batch 100: 0.34\n","loss at batch 200: 0.29\n","loss at batch 300: 0.38\n","loss at batch 400: 0.43\n","Epoch 16\n","loss at batch 0: 0.35\n","loss at batch 100: 0.34\n","loss at batch 200: 0.28\n","loss at batch 300: 0.37\n","loss at batch 400: 0.43\n","Epoch 17\n","loss at batch 0: 0.35\n","loss at batch 100: 0.33\n","loss at batch 200: 0.28\n","loss at batch 300: 0.37\n","loss at batch 400: 0.43\n","Epoch 18\n","loss at batch 0: 0.34\n","loss at batch 100: 0.33\n","loss at batch 200: 0.27\n","loss at batch 300: 0.36\n","loss at batch 400: 0.42\n","Epoch 19\n","loss at batch 0: 0.34\n","loss at batch 100: 0.32\n","loss at batch 200: 0.27\n","loss at batch 300: 0.36\n","loss at batch 400: 0.42\n","Epoch 20\n","loss at batch 0: 0.33\n","loss at batch 100: 0.32\n","loss at batch 200: 0.27\n","loss at batch 300: 0.36\n","loss at batch 400: 0.42\n","Epoch 21\n","loss at batch 0: 0.33\n","loss at batch 100: 0.32\n","loss at batch 200: 0.27\n","loss at batch 300: 0.36\n","loss at batch 400: 0.41\n","Epoch 22\n","loss at batch 0: 0.33\n","loss at batch 100: 0.31\n","loss at batch 200: 0.26\n","loss at batch 300: 0.35\n","loss at batch 400: 0.41\n","Epoch 23\n","loss at batch 0: 0.32\n","loss at batch 100: 0.31\n","loss at batch 200: 0.26\n","loss at batch 300: 0.35\n","loss at batch 400: 0.41\n","Epoch 24\n","loss at batch 0: 0.32\n","loss at batch 100: 0.31\n","loss at batch 200: 0.26\n","loss at batch 300: 0.35\n","loss at batch 400: 0.40\n","Epoch 25\n","loss at batch 0: 0.32\n","loss at batch 100: 0.30\n","loss at batch 200: 0.25\n","loss at batch 300: 0.34\n","loss at batch 400: 0.40\n","Epoch 26\n","loss at batch 0: 0.31\n","loss at batch 100: 0.30\n","loss at batch 200: 0.25\n","loss at batch 300: 0.34\n","loss at batch 400: 0.40\n","Epoch 27\n","loss at batch 0: 0.31\n","loss at batch 100: 0.30\n","loss at batch 200: 0.25\n","loss at batch 300: 0.34\n","loss at batch 400: 0.40\n","Epoch 28\n","loss at batch 0: 0.31\n","loss at batch 100: 0.29\n","loss at batch 200: 0.25\n","loss at batch 300: 0.34\n","loss at batch 400: 0.39\n","Epoch 29\n","loss at batch 0: 0.31\n","loss at batch 100: 0.29\n","loss at batch 200: 0.25\n","loss at batch 300: 0.34\n","loss at batch 400: 0.39\n","Epoch 30\n","loss at batch 0: 0.30\n","loss at batch 100: 0.29\n","loss at batch 200: 0.24\n","loss at batch 300: 0.33\n","loss at batch 400: 0.39\n","Epoch 31\n","loss at batch 0: 0.30\n","loss at batch 100: 0.29\n","loss at batch 200: 0.24\n","loss at batch 300: 0.33\n","loss at batch 400: 0.39\n","Epoch 32\n","loss at batch 0: 0.30\n","loss at batch 100: 0.28\n","loss at batch 200: 0.24\n","loss at batch 300: 0.33\n","loss at batch 400: 0.38\n","Epoch 33\n","loss at batch 0: 0.29\n","loss at batch 100: 0.28\n","loss at batch 200: 0.24\n","loss at batch 300: 0.33\n","loss at batch 400: 0.38\n","Epoch 34\n","loss at batch 0: 0.29\n","loss at batch 100: 0.28\n","loss at batch 200: 0.24\n","loss at batch 300: 0.33\n","loss at batch 400: 0.38\n","Epoch 35\n","loss at batch 0: 0.29\n","loss at batch 100: 0.28\n","loss at batch 200: 0.23\n","loss at batch 300: 0.32\n","loss at batch 400: 0.38\n","Epoch 36\n","loss at batch 0: 0.29\n","loss at batch 100: 0.28\n","loss at batch 200: 0.23\n","loss at batch 300: 0.32\n","loss at batch 400: 0.37\n","Epoch 37\n","loss at batch 0: 0.29\n","loss at batch 100: 0.27\n","loss at batch 200: 0.23\n","loss at batch 300: 0.32\n","loss at batch 400: 0.37\n","Epoch 38\n","loss at batch 0: 0.28\n","loss at batch 100: 0.27\n","loss at batch 200: 0.23\n","loss at batch 300: 0.32\n","loss at batch 400: 0.37\n","Epoch 39\n","loss at batch 0: 0.28\n","loss at batch 100: 0.27\n","loss at batch 200: 0.23\n","loss at batch 300: 0.32\n","loss at batch 400: 0.37\n","Epoch 40\n","loss at batch 0: 0.28\n","loss at batch 100: 0.27\n","loss at batch 200: 0.23\n","loss at batch 300: 0.32\n","loss at batch 400: 0.37\n","Epoch 41\n","loss at batch 0: 0.28\n","loss at batch 100: 0.27\n","loss at batch 200: 0.23\n","loss at batch 300: 0.31\n","loss at batch 400: 0.36\n","Epoch 42\n","loss at batch 0: 0.27\n","loss at batch 100: 0.26\n","loss at batch 200: 0.22\n","loss at batch 300: 0.31\n","loss at batch 400: 0.36\n","Epoch 43\n","loss at batch 0: 0.27\n","loss at batch 100: 0.26\n","loss at batch 200: 0.22\n","loss at batch 300: 0.31\n","loss at batch 400: 0.36\n","Epoch 44\n","loss at batch 0: 0.27\n","loss at batch 100: 0.26\n","loss at batch 200: 0.22\n","loss at batch 300: 0.31\n","loss at batch 400: 0.36\n","Epoch 45\n","loss at batch 0: 0.27\n","loss at batch 100: 0.26\n","loss at batch 200: 0.22\n","loss at batch 300: 0.31\n","loss at batch 400: 0.36\n","Epoch 46\n","loss at batch 0: 0.27\n","loss at batch 100: 0.26\n","loss at batch 200: 0.22\n","loss at batch 300: 0.31\n","loss at batch 400: 0.36\n","Epoch 47\n","loss at batch 0: 0.26\n","loss at batch 100: 0.26\n","loss at batch 200: 0.22\n","loss at batch 300: 0.31\n","loss at batch 400: 0.35\n","Epoch 48\n","loss at batch 0: 0.26\n","loss at batch 100: 0.26\n","loss at batch 200: 0.22\n","loss at batch 300: 0.30\n","loss at batch 400: 0.35\n","Epoch 49\n","loss at batch 0: 0.26\n","loss at batch 100: 0.25\n","loss at batch 200: 0.22\n","loss at batch 300: 0.30\n","loss at batch 400: 0.35\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9ZH-m1ySjAAe"},"source":["**exercise** \n","\n","1. Write a program to evaluate the accuracy of the model and evaluate the accuracy on both training and test datasets.\n","\n","2. Modify the code above so that you collect the gradients at each layer and each epoch.\n"]},{"cell_type":"markdown","metadata":{"id":"cbd7zbd1NW42"},"source":["## Additional resources\n","\n","- [Tensorflow, The Confusing Parts (1)](https://jacobbuckman.com/2018-06-25-tensorflow-the-confusing-parts-1/). Don't worry if some parts are still \"confusing\" after reading this."]}]}